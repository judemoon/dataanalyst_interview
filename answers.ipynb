{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interview Practice for Data Analyst Job \n",
    "\n",
    "August 2017, by Jude Moon\n",
    "\n",
    "## Practice Overview\n",
    "Employers use interviews to judge your readiness and fit for the job, which includes hearing about your skills and interest in the role. The interview is not a test or exam, but a conversation between you and the employer. Build your own strategies to be prepared come interview day. This project is one of many ways for you to practice!\n",
    "\n",
    "### 1. Describe a data project you worked on recently.\n",
    "\n",
    "One of the most challenging projects from the Data Analyst Nanodegree Program was a Machine Learning (ML) project for detecting fraud from Enron email and financial datasets. It was easy for me to explore and clean the data sets since I already had experience in data exploration from other ND projects and academia research. But the challenge was to figure out where to start among lots of algorithms available. The task was to build a person of interest (POI) identifier based on labeled data using Python. For supervised classification, I found 6 common algorithms from the scikit-learn library. I engineered and created 9 feature lists from the original feature list. To compare each pair of feature list and classifier algorithm, I had 60 combinations to test out. Instead of scripting procedures to perform all combinations, I stepped back and researched the pros, cons, and proper usages of each algorithm. And then I prioritized 3 top algorithms and feature lists. I documented the rationale behind the choice of algorithms. I constructed procedures and loops to implement pipeline and gridsearch for selecting features and optimizing hyper-parameters. I finally had a combination giving me good enough performance scores after about 20 trials. It could have been an exhausting trial process and could have taken me much longer time, but I could get it done within two days by spending some time to understand the algorithms first. \n",
    "\n",
    "From this, I have learned how to approach and plan for building a model when various options of algorithms and features are available. I think the knowledge and skills I gained from the ML course and project can be transferrable and lead to open the spectrum of my ability to build problem-solving and decision-making models on patient-related information and experience issues as well as clinic and hospital risks. Such models for identifying faulty information in electronic health record (EHR) and ineffective treatment using various features recorded from the past cases. If we successfully construct the models, they can be implemented in the EHR system to flag the errors real-time. The models are expected to tremendously improve patient service quality and reduce the hospital risks.\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "### 2. You are given a ten piece box of chocolate truffles. You know based on the label that six of the pieces have an orange cream filling and four of the pieces have a coconut filling. If you were to eat four pieces in a row, what is the probability that the first two pieces you eat have an orange cream filling and the last two have a coconut filling?\n",
    "\n",
    "* Probability of 1st and 2nd choices to be orange: $\\frac{6}{10} * \\frac{5}{9}$\n",
    "* Probability of 3rd and 4th choice to be coconut: $\\frac{4}{8} * \\frac{3}{7}$\n",
    "* Probability of 1st and 2nd choices to be orange and 3rd and 4th choice to be coconut: $\\frac{6}{10} * \\frac{5}{9} * \\frac{4}{8} * \\frac{3}{7} = 0.0714$\n",
    "\n",
    "\n",
    "### Follow up question: If you were given an identical box of chocolates and again eat four pieces in a row, what is the probability that exactly two contain coconut filling?\n",
    "\n",
    "* Number of cases to be orange or coconut for 4 pieces: $2 * 2 * 2 * 2 = 16$\n",
    "* Number of combinations to be two coconut and two orange: $(4-1) + (4-2) + (4-3) = 6$\n",
    "* Probability of combinations to be two coconut and two orange: $\\frac{6}{16} =0.375$\n",
    "\n",
    "----\n",
    "\n",
    "### 3. Given the table users:\n",
    "\n",
    "\n",
    "<center>Table \"users\"</center>\n",
    "\n",
    "\n",
    "| Column   | Type      |\n",
    "|----------|-----------|\n",
    "| id       | integer   |\n",
    "| username | character |\n",
    "| email    | character |\n",
    "| city     | character |\n",
    "| state    | character |\n",
    "| zip      | integer   |\n",
    "| active   | boolean   |\n",
    "\n",
    "### Construct a query to find the top 5 states with the highest number of active users. Include the number for each state in the query result. Example result:\n",
    "\n",
    "| state      | num_active_users |\n",
    "|------------|------------------|\n",
    "| New Mexico | 502              |\n",
    "| Alabama    | 495              |\n",
    "| California | 300              |\n",
    "| Maine      | 201              |\n",
    "| Texas      | 189              |\n",
    "\n",
    "\n",
    "    SELECT state, SUM(active)\n",
    "    from users\n",
    "    GROUP BY state\n",
    "    ORDER BY SUM(active) DESC\n",
    "    LIMIT 5\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "### 4. Define a function first_unique that takes a string as input and returns the first non-repeated (unique) character in the input string. If there are no unique characters return None. Note: Your code should be in Python.\n",
    "\n",
    "```\n",
    "def first_unique(string):\n",
    " # Your code here\n",
    " return unique_char\n",
    "\n",
    "> first_unique('aabbcdd123')\n",
    "> c\n",
    "\n",
    "> first_unique('a')\n",
    "> a\n",
    "\n",
    "> first_unique('112233')\n",
    "> None\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def first_unique(word):\n",
    "    \n",
    "    counts = defaultdict(int) # initiate defaultdict for count \n",
    "    \n",
    "    for c in word:\n",
    "        counts[c] += 1 # add c as a key and number as a value to dictionary\n",
    "      \n",
    "    for c in counts:\n",
    "        if counts[c] == 1: # if the value is 1 (appeared once), return the key\n",
    "            return c\n",
    "    \n",
    "    return \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_unique('aabbcdd123')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_unique('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'None'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_unique('112233')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### 5. What are underfitting and overfitting in the context of Machine Learning? How might you balance them?\n",
    "\n",
    "Both underfitting and overfitting refer poor generalization to new data, but underfitting refers poor performance on the training data as well, while overfitting refers good performance on the training data. Underfitting is easy to be detected by evaluating metrics and it is better to move on and try other algorithms. Overfitting can be controlled by adjusting parameters, so that it limits how much detail and noise in the training data the model would learn.\n",
    "\n",
    "To balance between good performance on training data and unseen data, cross validation can be used. We can separate a subset of the training data and hold back from our machine learning algorithm training and tuning. After finishing selecting and tuning algorithms, we can get evaluation of the algorithm on the subset, which shows a performance on unseen data. But if we have a limited dataset size, such methods like KFold and ShuffleSplit allow to cross evaluate the algorithm on multiple different splitting sets of training and testing data and give average evaluating metrics.\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "### 6. If you were to start your data analyst position today, what would be your goals a year from now?\n",
    "\n",
    "I would like to work on managing health care database and data system in HealthTech Hub community in Austin, TX. My goal is to fully understand health related database system and metrics and provide high-quality reporting to improve service administration and patient experience.\n",
    "\n",
    "One-year Goals:\n",
    "\n",
    "1.\tUnderstanding current database designs and metrics and becoming proficient in managing them\n",
    "2.\tDeveloping and implementing databases, data collection systems, and data metrics and reporting standards\n",
    "3.\tBuilding models and conducting analyses on patient experience to support 3-year strategic agenda goals\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
